[2025-09-03T00:23:17.335+0000] {processor.py:153} INFO - Started process (PID=14611) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:17.336+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:23:17.336+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:17.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:17.354+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:17.373+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:17.373+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:23:17.388+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:17.388+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:23:17.402+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T00:23:47.516+0000] {processor.py:153} INFO - Started process (PID=14621) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:47.517+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:23:47.517+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:47.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:47.531+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:23:47.546+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:47.546+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:23:47.560+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:23:47.560+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:23:47.572+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T00:24:17.796+0000] {processor.py:153} INFO - Started process (PID=15126) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:17.837+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:24:17.840+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:17.840+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:19.168+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:20.320+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:20.314+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:24:20.408+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:20.407+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:24:20.445+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 2.856 seconds
[2025-09-03T00:24:51.226+0000] {processor.py:153} INFO - Started process (PID=16199) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:51.227+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:24:51.228+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:51.228+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:51.246+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:51.267+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:51.266+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:24:51.284+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:51.284+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:24:51.300+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.078 seconds
[2025-09-03T00:25:42.884+0000] {processor.py:153} INFO - Started process (PID=16221) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:42.884+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:25:42.885+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:42.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:42.901+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:42.918+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:42.918+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:25:42.933+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:42.933+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:25:42.946+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T00:24:56.353+0000] {processor.py:153} INFO - Started process (PID=16250) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:56.354+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:24:56.354+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:56.354+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:56.368+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:56.380+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:56.380+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_fail at 0x7f35c736ca60>
[2025-09-03T00:24:56.386+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T00:24:56.796+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:56.796+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:24:56.807+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:56.806+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:24:56.819+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.468 seconds
[2025-09-03T00:25:48.011+0000] {processor.py:153} INFO - Started process (PID=16255) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:48.012+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:25:48.012+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:48.012+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:48.027+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:24:57.396+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:57.396+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:24:57.409+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:24:57.409+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:24:57.430+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T00:25:53.038+0000] {processor.py:153} INFO - Started process (PID=16260) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:53.039+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:25:53.039+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:53.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:02.402+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:02.421+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:02.421+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:25:02.438+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:02.438+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:25:02.453+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T00:25:28.537+0000] {processor.py:153} INFO - Started process (PID=16265) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:28.538+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:25:28.538+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:28.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:28.553+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:28.567+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:28.567+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:25:28.582+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:28.582+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:25:28.596+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T00:25:58.652+0000] {processor.py:153} INFO - Started process (PID=16270) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:58.653+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:25:58.653+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:58.653+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:58.668+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:25:58.684+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:58.684+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:25:58.698+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:25:58.698+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:25:58.708+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T00:26:28.810+0000] {processor.py:153} INFO - Started process (PID=16280) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:28.810+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:26:28.811+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:28.811+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:28.824+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:28.839+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:28.839+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:26:28.852+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:28.852+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:26:28.863+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T00:26:58.968+0000] {processor.py:153} INFO - Started process (PID=16290) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:58.968+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:26:58.968+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:58.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:58.983+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:26:58.998+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:58.998+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:26:59.011+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:26:59.011+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:26:59.023+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T00:27:29.145+0000] {processor.py:153} INFO - Started process (PID=16300) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:29.146+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:27:29.146+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:29.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:29.161+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:29.177+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:29.177+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:27:29.191+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:29.191+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:27:29.202+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T00:27:59.316+0000] {processor.py:153} INFO - Started process (PID=16310) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:59.316+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:27:59.317+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:59.317+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:59.331+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:27:59.347+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:59.347+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:27:59.361+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:27:59.361+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:27:59.371+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T00:29:08.192+0000] {processor.py:153} INFO - Started process (PID=16326) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:08.192+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:29:08.193+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:08.193+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:08.209+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:08.225+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:08.225+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:29:08.240+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:08.240+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:29:08.251+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T00:28:47.752+0000] {processor.py:153} INFO - Started process (PID=17817) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:47.753+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:28:47.753+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:47.753+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:47.887+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:47.951+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:47.951+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:28:47.971+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:47.970+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:28:47.983+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.233 seconds
[2025-09-03T00:28:57.789+0000] {processor.py:153} INFO - Started process (PID=17920) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:57.790+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:28:57.790+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:57.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:57.804+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:28:57.816+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:57.816+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7f35c7383820>
[2025-09-03T00:28:57.822+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T00:28:57.836+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:57.835+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:28:57.847+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:28:57.846+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:28:57.857+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T00:29:27.948+0000] {processor.py:153} INFO - Started process (PID=17931) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:27.949+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:29:27.950+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:27.950+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:27.969+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:27.987+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:27.987+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:29:28.004+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:28.004+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:29:28.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.074 seconds
[2025-09-03T00:29:58.124+0000] {processor.py:153} INFO - Started process (PID=17941) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:58.125+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:29:58.125+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:58.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:58.164+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:29:58.206+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:58.206+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:29:58.223+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:29:58.223+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:29:58.235+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.114 seconds
[2025-09-03T00:30:28.377+0000] {processor.py:153} INFO - Started process (PID=17953) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:28.378+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:30:28.378+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:28.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:28.422+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:28.451+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:28.451+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:30:28.465+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:28.465+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:30:28.479+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.105 seconds
[2025-09-03T00:30:58.583+0000] {processor.py:153} INFO - Started process (PID=17963) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:58.584+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:30:58.585+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:58.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:58.622+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:30:58.658+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:58.658+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:30:58.672+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:30:58.672+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:30:58.683+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.103 seconds
[2025-09-03T00:32:13.415+0000] {processor.py:153} INFO - Started process (PID=17968) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:13.416+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:32:13.417+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:13.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:13.481+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:13.524+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:13.524+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:32:13.540+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:13.540+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:32:13.552+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.139 seconds
[2025-09-03T00:32:48.552+0000] {processor.py:153} INFO - Started process (PID=17978) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:48.553+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:32:48.556+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:48.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:31:58.203+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:31:58.354+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:31:58.353+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:31:58.425+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:31:58.425+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:31:58.462+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.560 seconds
[2025-09-03T00:32:28.548+0000] {processor.py:153} INFO - Started process (PID=17988) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:28.548+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:32:28.549+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:28.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:28.565+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:28.584+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:28.584+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:32:28.600+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:28.600+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:32:28.612+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T00:32:59.411+0000] {processor.py:153} INFO - Started process (PID=17998) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:59.413+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:32:59.414+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:59.414+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:59.490+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:32:59.547+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:59.547+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:32:59.573+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:32:59.572+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:32:59.591+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.185 seconds
[2025-09-03T00:33:29.731+0000] {processor.py:153} INFO - Started process (PID=18008) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:33:29.732+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:33:29.733+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:33:29.732+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:33:29.809+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:33:29.846+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:33:29.846+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:33:29.861+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:33:29.861+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:33:29.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.160 seconds
[2025-09-03T00:33:59.989+0000] {processor.py:153} INFO - Started process (PID=18018) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:33:59.989+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:33:59.990+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:33:59.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:00.008+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:00.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:00.028+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:34:00.045+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:00.045+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:34:00.065+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.080 seconds
[2025-09-03T00:34:53.742+0000] {processor.py:153} INFO - Started process (PID=18028) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:53.742+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:34:53.742+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:53.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:53.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:53.774+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:53.774+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:34:53.790+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:53.790+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:34:53.804+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T00:34:33.273+0000] {processor.py:153} INFO - Started process (PID=18038) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:33.273+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:34:33.274+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:33.274+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:33.293+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:33.313+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:33.313+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:34:33.345+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:33.345+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:34:33.366+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.096 seconds
[2025-09-03T00:34:38.341+0000] {processor.py:153} INFO - Started process (PID=18048) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:38.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:34:38.347+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:38.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:38.460+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:34:38.481+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:38.481+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:34:38.499+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:34:38.499+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:34:38.515+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.194 seconds
[2025-09-03T00:35:08.639+0000] {processor.py:153} INFO - Started process (PID=18058) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:08.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:35:08.640+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:08.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:08.662+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:08.681+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:08.681+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:35:08.696+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:08.696+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:35:08.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.077 seconds
[2025-09-03T00:35:38.825+0000] {processor.py:153} INFO - Started process (PID=18068) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:38.826+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:35:38.826+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:38.826+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:38.842+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:35:38.862+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:38.862+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:35:38.877+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:35:38.877+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:35:38.891+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.070 seconds
[2025-09-03T00:36:09.026+0000] {processor.py:153} INFO - Started process (PID=18078) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:09.027+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:36:09.027+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:09.027+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:09.045+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:09.062+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:09.062+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:36:09.076+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:09.076+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:36:09.087+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T00:36:41.944+0000] {processor.py:153} INFO - Started process (PID=18088) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:41.950+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:36:41.955+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:41.954+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:42.234+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:42.321+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:42.321+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:36:42.406+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:42.405+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:36:42.445+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.532 seconds
[2025-09-03T00:36:48.490+0000] {processor.py:153} INFO - Started process (PID=18098) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:48.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:36:48.492+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:48.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:48.508+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:36:48.524+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:48.524+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:36:48.538+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:36:48.538+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:36:48.549+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T00:37:18.674+0000] {processor.py:153} INFO - Started process (PID=18108) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:18.675+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:37:18.676+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:18.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:18.695+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:18.716+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:18.716+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:37:18.735+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:18.735+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:37:18.755+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.084 seconds
[2025-09-03T00:37:48.905+0000] {processor.py:153} INFO - Started process (PID=18118) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:48.906+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:37:48.906+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:48.906+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:48.953+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:37:48.971+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:48.971+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:37:48.988+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:37:48.988+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:37:49.001+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T00:38:19.089+0000] {processor.py:153} INFO - Started process (PID=18128) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:19.089+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:38:19.090+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:19.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:19.115+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:19.149+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:19.148+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:38:19.169+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:19.169+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:38:19.184+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.100 seconds
[2025-09-03T00:38:49.275+0000] {processor.py:153} INFO - Started process (PID=18138) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:49.278+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:38:49.279+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:49.279+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:49.377+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:38:49.427+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:49.427+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:38:49.443+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:38:49.442+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:38:49.461+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.194 seconds
[2025-09-03T00:39:20.215+0000] {processor.py:153} INFO - Started process (PID=18148) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:20.216+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:39:20.217+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:20.217+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:20.298+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:20.331+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:20.330+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:39:20.346+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:20.346+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:39:20.363+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.150 seconds
[2025-09-03T00:39:50.475+0000] {processor.py:153} INFO - Started process (PID=18158) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:50.476+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:39:50.476+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:50.476+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:50.493+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:39:50.510+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:50.509+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:39:50.525+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:39:50.525+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:39:50.537+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T00:40:44.217+0000] {processor.py:153} INFO - Started process (PID=18168) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:44.218+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:40:44.218+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:44.218+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:44.234+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:44.251+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:44.251+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:40:44.267+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:44.267+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:40:44.280+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T00:41:14.398+0000] {processor.py:153} INFO - Started process (PID=18178) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:14.399+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:41:14.400+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:14.400+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:14.421+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:14.442+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:14.442+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:40:23.829+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:23.829+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:40:23.847+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.088 seconds
[2025-09-03T00:40:53.980+0000] {processor.py:153} INFO - Started process (PID=18188) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:53.980+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:40:53.981+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:53.981+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:54.008+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:40:54.035+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:54.035+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:40:54.059+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:40:54.059+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:40:54.073+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T00:41:24.223+0000] {processor.py:153} INFO - Started process (PID=18198) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:24.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:41:24.227+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:24.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:24.393+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:24.473+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:24.473+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:41:24.543+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:24.543+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:41:24.560+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.346 seconds
[2025-09-03T00:41:54.746+0000] {processor.py:153} INFO - Started process (PID=18208) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:54.746+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:41:54.747+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:54.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:54.774+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:41:54.805+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:54.805+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:41:54.822+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:41:54.822+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:41:54.835+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.122 seconds
[2025-09-03T00:42:49.509+0000] {processor.py:153} INFO - Started process (PID=18218) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:42:49.509+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:42:49.510+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:42:49.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:42:49.525+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:42:49.541+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:42:49.541+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:42:49.555+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:42:49.555+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:42:49.569+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T00:43:20.222+0000] {processor.py:153} INFO - Started process (PID=18228) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:20.223+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:43:20.224+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:20.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:20.273+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:20.306+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:20.306+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:43:20.331+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:20.330+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:43:20.344+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.126 seconds
[2025-09-03T00:43:50.481+0000] {processor.py:153} INFO - Started process (PID=18238) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:50.482+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:43:50.483+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:50.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:50.503+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:43:50.524+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:50.524+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:43:50.538+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:43:50.538+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:43:50.549+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.071 seconds
[2025-09-03T00:44:20.576+0000] {processor.py:153} INFO - Started process (PID=18248) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:20.577+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:44:20.577+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:20.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:20.593+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:20.609+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:20.609+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:44:20.623+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:20.623+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:44:20.633+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T00:44:50.778+0000] {processor.py:153} INFO - Started process (PID=18258) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:50.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:44:50.779+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:50.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:50.825+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:44:50.858+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:50.858+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:44:50.875+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:44:50.875+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:44:50.887+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.111 seconds
[2025-09-03T00:45:20.984+0000] {processor.py:153} INFO - Started process (PID=18268) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:45:20.984+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:45:20.985+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:45:20.985+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:45:21.000+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:45:21.016+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:45:21.016+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:45:21.033+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:45:21.033+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:45:21.044+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T00:46:14.726+0000] {processor.py:153} INFO - Started process (PID=18278) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:14.726+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:46:14.726+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:14.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:14.742+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:14.758+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:14.758+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:46:14.773+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:14.773+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:46:14.784+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T00:46:44.904+0000] {processor.py:153} INFO - Started process (PID=18288) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:44.904+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:46:44.905+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:44.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:44.923+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:45:54.293+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:45:54.292+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:45:54.312+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:45:54.311+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:45:54.328+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T00:46:24.461+0000] {processor.py:153} INFO - Started process (PID=18298) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:24.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:46:24.464+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:24.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:24.488+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:24.515+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:24.515+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:46:24.533+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:24.533+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:46:24.549+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.094 seconds
[2025-09-03T00:46:54.580+0000] {processor.py:153} INFO - Started process (PID=18308) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:54.580+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:46:54.581+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:54.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:54.600+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:46:54.626+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:54.626+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:46:54.654+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:46:54.654+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:46:54.675+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T00:47:25.705+0000] {processor.py:153} INFO - Started process (PID=18319) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:47:25.707+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:47:25.711+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:47:25.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:47:26.018+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:47:26.147+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:47:26.146+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:47:26.231+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:47:26.231+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:47:26.261+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.577 seconds
[2025-09-03T00:48:19.929+0000] {processor.py:153} INFO - Started process (PID=18329) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:19.930+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:48:19.930+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:19.930+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:19.947+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:19.963+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:19.963+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:48:19.977+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:19.977+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:48:19.988+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T00:48:50.116+0000] {processor.py:153} INFO - Started process (PID=18341) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:50.119+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:48:50.119+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:50.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:50.136+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:48:50.153+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:50.153+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:48:50.168+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:48:50.168+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:48:50.179+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T00:49:20.826+0000] {processor.py:153} INFO - Started process (PID=18351) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:20.827+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:49:20.828+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:20.828+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:20.868+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:20.912+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:20.912+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:49:20.944+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:20.944+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:49:20.960+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.140 seconds
[2025-09-03T00:49:51.072+0000] {processor.py:153} INFO - Started process (PID=18361) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:51.072+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:49:51.073+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:51.073+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:51.094+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:49:51.116+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:51.115+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:49:51.133+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:49:51.133+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:49:51.150+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.082 seconds
[2025-09-03T00:50:21.282+0000] {processor.py:153} INFO - Started process (PID=18371) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:21.283+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:50:21.283+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:21.283+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:21.322+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:21.343+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:21.343+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:50:21.369+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:21.369+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:50:21.389+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.111 seconds
[2025-09-03T00:50:51.553+0000] {processor.py:153} INFO - Started process (PID=18381) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:51.555+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:50:51.558+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:51.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:51.796+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:50:51.876+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:51.876+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:50:51.984+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:50:51.984+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:50:52.007+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.481 seconds
[2025-09-03T00:51:22.169+0000] {processor.py:153} INFO - Started process (PID=18391) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:22.172+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:51:22.175+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:22.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:22.340+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:22.424+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:22.423+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:51:22.494+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:22.494+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:51:22.532+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.383 seconds
[2025-09-03T00:51:52.635+0000] {processor.py:153} INFO - Started process (PID=18401) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:52.636+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:51:52.637+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:52.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:52.654+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:51:52.671+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:52.671+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:51:52.685+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:51:52.685+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:51:52.697+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T00:52:45.363+0000] {processor.py:153} INFO - Started process (PID=18411) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:45.364+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:52:45.364+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:45.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:45.381+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:45.397+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:45.397+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:52:45.411+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:45.411+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:52:45.424+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T00:53:15.543+0000] {processor.py:153} INFO - Started process (PID=18421) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:15.543+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:53:15.544+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:15.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:24.929+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:24.950+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:24.950+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:52:24.966+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:24.965+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:52:24.985+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.076 seconds
[2025-09-03T00:52:55.089+0000] {processor.py:153} INFO - Started process (PID=18431) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:55.090+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:52:55.090+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:55.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:55.115+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:52:55.155+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:55.155+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:52:55.171+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:52:55.171+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:52:55.185+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.100 seconds
[2025-09-03T00:53:25.203+0000] {processor.py:153} INFO - Started process (PID=18441) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:25.204+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:53:25.204+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:25.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:25.219+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:25.236+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:25.236+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:53:25.250+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:25.250+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:53:25.263+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T00:53:55.392+0000] {processor.py:153} INFO - Started process (PID=18452) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:55.392+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:53:55.393+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:55.393+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:55.424+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:53:55.458+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:55.458+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:53:55.479+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:53:55.479+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:53:55.495+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.108 seconds
[2025-09-03T00:54:25.595+0000] {processor.py:153} INFO - Started process (PID=18462) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:25.596+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:54:25.596+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:25.596+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:25.612+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:25.629+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:25.629+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:54:25.643+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:25.643+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:54:25.657+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.064 seconds
[2025-09-03T00:54:55.779+0000] {processor.py:153} INFO - Started process (PID=18472) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:55.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:54:55.779+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:55.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:55.810+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:54:55.839+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:55.839+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:54:55.858+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:54:55.858+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:54:55.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T00:55:25.963+0000] {processor.py:153} INFO - Started process (PID=18483) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:55:25.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:55:25.964+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:55:25.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:55:25.981+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:55:25.999+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:55:25.998+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:55:26.013+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:55:26.013+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:55:26.024+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.064 seconds
[2025-09-03T00:56:20.709+0000] {processor.py:153} INFO - Started process (PID=18493) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:20.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:56:20.710+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:20.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:20.725+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:20.741+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:20.741+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:56:20.755+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:20.755+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:56:20.765+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T00:56:50.881+0000] {processor.py:153} INFO - Started process (PID=18503) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:50.882+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:56:50.882+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:50.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:50.903+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:50.925+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:50.925+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:56:00.297+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:00.297+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:56:00.309+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T00:56:05.276+0000] {processor.py:153} INFO - Started process (PID=18513) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:05.280+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:56:05.281+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:05.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:05.299+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:05.317+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:05.317+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:56:05.331+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:05.331+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:56:05.341+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.068 seconds
[2025-09-03T00:56:35.470+0000] {processor.py:153} INFO - Started process (PID=18523) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:35.470+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:56:35.471+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:35.471+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:35.507+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:56:35.524+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:35.524+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:56:35.540+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:56:35.539+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:56:35.551+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.084 seconds
[2025-09-03T00:57:05.578+0000] {processor.py:153} INFO - Started process (PID=18533) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:05.579+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:57:05.580+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:05.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:05.595+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:05.611+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:05.611+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:57:05.625+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:05.625+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:57:05.636+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T00:57:35.753+0000] {processor.py:153} INFO - Started process (PID=18545) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:35.754+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:57:35.754+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:35.754+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:35.774+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:57:35.794+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:35.794+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:57:35.812+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:57:35.812+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:57:35.827+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.077 seconds
[2025-09-03T00:58:05.878+0000] {processor.py:153} INFO - Started process (PID=18555) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:05.879+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:58:05.879+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:05.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:05.930+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:05.973+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:05.973+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:58:05.988+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:05.988+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:58:05.999+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.124 seconds
[2025-09-03T00:58:36.105+0000] {processor.py:153} INFO - Started process (PID=18565) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:36.105+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:58:36.106+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:36.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:36.121+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:58:36.139+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:36.138+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:58:36.154+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:58:36.154+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:58:36.165+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T00:59:06.286+0000] {processor.py:153} INFO - Started process (PID=18575) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:59:06.287+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T00:59:06.287+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:59:06.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:59:06.303+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T00:59:06.320+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:59:06.320+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:59:06.335+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:59:06.334+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:59:06.345+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T01:00:01.005+0000] {processor.py:153} INFO - Started process (PID=18585) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:01.005+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:00:01.005+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:01.005+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:01.022+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:01.040+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:01.040+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:00:01.054+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:01.054+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:00:01.065+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:00:31.185+0000] {processor.py:153} INFO - Started process (PID=18595) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:31.185+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:00:31.185+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:31.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:31.201+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:31.216+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:31.216+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T00:59:40.590+0000] {logging_mixin.py:137} INFO - [2025-09-03T00:59:40.590+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T00:59:40.600+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:00:10.716+0000] {processor.py:153} INFO - Started process (PID=18605) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:10.717+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:00:10.717+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:10.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:10.732+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:00:10.747+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:10.747+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:00:10.760+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:00:10.759+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:00:10.770+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T01:02:47.140+0000] {processor.py:153} INFO - Started process (PID=229) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:02:47.141+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:02:47.141+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:47.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:02:47.306+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:02:47.323+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:02:47.329+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:47.323+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 2, 17, 314812, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:02:47.330+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:47.330+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:47.773+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:47.773+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 2, 17, 772506, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:02:47.774+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:47.774+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:48.628+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:48.628+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 2, 18, 627821, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:02:48.629+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:48.629+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:48.630+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:03:41.283+0000] {processor.py:153} INFO - Started process (PID=244) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:41.286+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:03:41.286+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:41.286+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:41.299+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:41.310+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:03:41.312+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:41.310+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 3, 11, 300692, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:03:41.313+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:41.312+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:50.966+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:50.965+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 2, 20, 964676, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:02:50.966+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:50.966+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:51.768+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:51.767+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 2, 21, 766761, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:02:51.768+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:02:51.768+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:02:51.769+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:03:46.307+0000] {processor.py:153} INFO - Started process (PID=254) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:46.307+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:03:46.307+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.307+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:46.321+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:03:46.391+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.391+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:03:46.399+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.399+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:03:46.405+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.405+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:03:46.405+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.405+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:03:46.412+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.412+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T01:03:46.417+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:46.417+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:03:46.428+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.124 seconds
[2025-09-03T01:04:26.453+0000] {processor.py:153} INFO - Started process (PID=274) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:26.454+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:04:26.454+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:26.454+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:26.469+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:26.479+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:04:26.481+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:26.479+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 3, 56, 470117, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:04:26.481+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:26.481+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:03:36.160+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:36.160+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 3, 6, 159567, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:03:36.161+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:36.161+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:03:36.178+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:36.177+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 3, 6, 177041, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:03:36.178+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:03:36.178+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:03:36.179+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:04:06.357+0000] {processor.py:153} INFO - Started process (PID=284) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:06.358+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:04:06.359+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:06.358+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:06.373+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:06.383+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:06.383+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:04:06.396+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:06.396+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:04:06.409+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.053 seconds
[2025-09-03T01:04:37.425+0000] {processor.py:153} INFO - Started process (PID=294) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:37.426+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:04:37.426+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:37.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:37.439+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:04:37.455+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:37.455+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:04:37.468+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:04:37.468+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:04:37.479+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:05:07.527+0000] {processor.py:153} INFO - Started process (PID=304) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:07.528+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:05:07.528+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:07.528+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:07.541+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:07.556+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:07.556+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:05:07.569+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:07.569+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:05:07.580+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.054 seconds
[2025-09-03T01:05:37.712+0000] {processor.py:153} INFO - Started process (PID=314) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:37.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:05:37.713+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:37.713+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:37.726+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:05:37.735+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:05:37.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:37.736+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 7, 727058, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:05:37.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:37.738+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:05:37.759+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:37.758+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 7, 758179, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:05:37.759+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:37.759+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:05:38.536+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:38.535+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 8, 535253, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:05:38.536+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:05:38.536+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:05:38.538+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:08.642+0000] {processor.py:153} INFO - Started process (PID=326) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:08.642+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:06:08.643+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:08.643+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:08.656+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:08.667+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:06:08.670+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:08.667+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 38, 657428, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:06:08.670+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:08.670+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:08.944+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:08.943+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 38, 942919, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:08.944+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:08.944+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:09.043+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:09.042+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 39, 42088, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:09.043+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:09.043+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:09.044+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:01.712+0000] {processor.py:153} INFO - Started process (PID=336) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:01.712+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:07:01.712+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:01.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:01.726+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:01.736+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:07:01.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:01.736+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 31, 727239, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:07:01.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:01.738+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:01.758+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:01.757+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 31, 757285, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:01.758+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:01.758+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:11.662+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:11.661+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 5, 41, 661254, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:11.662+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:11.662+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:11.663+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:41.778+0000] {processor.py:153} INFO - Started process (PID=346) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:41.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:06:41.779+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:41.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:41.792+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:06:41.802+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:06:41.805+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:41.802+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 11, 793543, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:06:41.805+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:41.805+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:42.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:42.028+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 12, 27215, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:42.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:42.028+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:42.475+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:42.474+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 12, 473947, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:06:42.475+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:06:42.475+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:06:42.477+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:12.591+0000] {processor.py:153} INFO - Started process (PID=356) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:12.595+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:07:12.595+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:12.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:12.615+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:12.628+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:07:12.632+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:12.629+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 42, 616693, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:07:12.633+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:12.632+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:12.689+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:12.689+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 42, 688626, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:12.690+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:12.690+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:13.424+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:13.423+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 6, 43, 423049, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:13.424+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:13.424+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:13.425+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:43.562+0000] {processor.py:153} INFO - Started process (PID=366) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:43.562+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:07:43.563+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:43.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:43.577+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:07:43.588+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:07:43.592+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:43.588+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 13, 578632, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:07:43.592+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:43.592+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:43.668+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:43.667+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 13, 666990, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:43.668+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:43.668+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:44.070+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:44.070+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 14, 69458, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:07:44.071+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:07:44.071+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:07:44.072+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:08:14.801+0000] {processor.py:153} INFO - Started process (PID=376) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:14.801+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:08:14.802+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:14.802+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:14.819+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:14.832+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:08:14.835+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:14.832+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 44, 820926, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:08:14.836+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:14.836+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:08:15.165+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:15.164+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 45, 163886, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:08:15.165+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:15.165+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:08:15.906+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:15.905+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 7, 45, 905016, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:08:15.906+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:15.906+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:08:15.907+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:08:45.999+0000] {processor.py:153} INFO - Started process (PID=391) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:46.000+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:08:46.000+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:46.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:46.013+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:08:46.024+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:08:46.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:46.024+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 8, 16, 14196, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:08:46.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:46.028+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:09:37.015+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:09:37.015+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 9, 7, 13488, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:09:37.015+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:09:37.015+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:08:47.209+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:47.208+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 8, 17, 208143, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:08:47.209+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:08:47.209+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:08:47.210+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:09:41.880+0000] {processor.py:153} INFO - Started process (PID=396) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:09:41.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:09:41.881+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:09:41.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:09:41.897+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:09:41.936+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:09:41.935+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:09:41.949+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:09:41.949+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:09:41.962+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.084 seconds
[2025-09-03T01:10:12.003+0000] {processor.py:153} INFO - Started process (PID=421) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:12.004+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:10:12.004+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:12.004+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:12.018+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:12.033+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:12.033+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:10:12.046+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:12.046+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:10:12.061+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:10:42.161+0000] {processor.py:153} INFO - Started process (PID=431) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:42.162+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:10:42.162+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:42.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:42.178+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:10:42.201+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:42.201+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:10:42.216+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:10:42.216+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:10:42.233+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T01:11:12.250+0000] {processor.py:153} INFO - Started process (PID=441) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:11:12.250+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:11:12.250+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.250+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:11:12.267+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:11:12.279+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:11:12.282+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.279+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 10, 42, 268264, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:11:12.282+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.282+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:12.630+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.630+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 10, 42, 629286, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:11:12.630+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.630+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:12.891+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.891+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 10, 42, 890605, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:11:12.892+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:12.892+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:12.893+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:12:22.129+0000] {processor.py:153} INFO - Started process (PID=451) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:22.130+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:12:22.130+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:22.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:22.143+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:22.155+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:12:22.157+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:22.155+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 11, 52, 144117, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:12:22.157+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:22.157+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:31.793+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:31.792+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 11, 1, 792143, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:11:31.793+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:31.793+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:32.726+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:32.726+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 11, 2, 725463, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:11:32.727+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:11:32.727+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:11:32.728+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:12:27.149+0000] {processor.py:153} INFO - Started process (PID=461) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:27.150+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:12:27.150+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:27.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:27.163+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:27.178+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:27.177+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:12:27.191+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:27.191+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:12:27.204+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:13:02.196+0000] {processor.py:153} INFO - Started process (PID=477) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:02.198+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:13:02.199+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:02.198+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:02.212+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:02.223+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:13:02.226+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:02.223+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 12, 32, 213907, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:13:02.227+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:02.227+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:12:12.025+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:12.025+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 11, 42, 24289, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:12:12.026+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:12.025+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:12:12.216+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:12.216+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 11, 42, 215755, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:12:12.217+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:12.217+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:12:12.218+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:13:07.214+0000] {processor.py:153} INFO - Started process (PID=482) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:07.214+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:13:07.214+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:07.214+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:07.228+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:07.244+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:07.244+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:13:07.257+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:07.257+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:13:07.270+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:13:37.391+0000] {processor.py:153} INFO - Started process (PID=497) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:37.394+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:13:37.395+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:37.395+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:37.409+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:12:46.803+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:46.803+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:12:46.816+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:12:46.816+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:12:46.833+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T01:13:16.911+0000] {processor.py:153} INFO - Started process (PID=502) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:16.911+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:13:16.911+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:16.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:16.924+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:16.943+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:16.943+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:13:16.956+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:16.956+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:13:16.967+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T01:13:47.053+0000] {processor.py:153} INFO - Started process (PID=512) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:47.054+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:13:47.054+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:47.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:47.068+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:13:47.084+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:47.084+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:13:47.099+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:13:47.099+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:13:47.114+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:14:17.217+0000] {processor.py:153} INFO - Started process (PID=522) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:17.217+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:14:17.218+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:17.218+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:17.231+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:17.245+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:17.245+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:14:17.259+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:17.258+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:14:17.272+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:14:47.360+0000] {processor.py:153} INFO - Started process (PID=532) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:47.360+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:14:47.361+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:47.361+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:47.374+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:14:47.391+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:47.391+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:14:47.404+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:14:47.404+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:14:47.418+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:15:17.522+0000] {processor.py:153} INFO - Started process (PID=542) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:17.523+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:15:17.523+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:17.523+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:17.536+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:17.552+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:17.551+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:15:17.565+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:17.565+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:15:17.577+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:15:47.608+0000] {processor.py:153} INFO - Started process (PID=552) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:47.609+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:15:47.610+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:47.610+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:47.623+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:15:47.638+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:47.638+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:15:47.650+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:15:47.650+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:15:47.665+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T01:16:17.761+0000] {processor.py:153} INFO - Started process (PID=557) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:17.762+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:16:17.762+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:17.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:17.776+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:17.786+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:16:17.789+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:17.786+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 15, 47, 777088, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:16:17.789+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:17.789+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:18.292+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:18.292+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 15, 48, 291661, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:18.293+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:18.293+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:18.949+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:18.948+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 15, 48, 948196, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:18.949+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:18.949+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:18.950+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:12.611+0000] {processor.py:153} INFO - Started process (PID=567) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:12.611+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:17:12.611+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:12.611+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:12.624+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:12.639+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:12.639+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:12.651+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:12.651+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:17:12.664+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T01:17:42.784+0000] {processor.py:153} INFO - Started process (PID=577) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:42.785+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:17:42.785+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:42.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:52.177+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:52.188+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:16:52.190+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:52.188+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 22, 178307, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:16:52.191+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:52.191+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:52.518+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:52.517+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 22, 516945, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:52.518+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:52.518+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:53.042+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:53.041+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 23, 41148, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:53.042+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:53.042+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:53.043+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:47.808+0000] {processor.py:153} INFO - Started process (PID=587) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:47.808+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:17:47.808+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:47.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:47.821+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:16:57.199+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:16:57.202+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.199+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 17, 821928, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:16:57.202+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.202+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:57.693+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.692+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 27, 691900, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:57.693+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.693+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:57.867+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.866+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 27, 866181, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:16:57.867+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:16:57.867+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:16:57.868+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:27.913+0000] {processor.py:153} INFO - Started process (PID=600) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:27.914+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:17:27.914+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:27.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:27.926+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:27.936+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:17:27.938+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:27.936+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 57, 927875, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:17:27.939+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:27.939+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:28.308+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:28.307+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 58, 306568, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:28.308+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:28.308+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:28.457+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:28.456+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 16, 58, 456287, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:28.457+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:28.457+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:28.458+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:58.572+0000] {processor.py:153} INFO - Started process (PID=607) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:58.572+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:17:58.573+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.573+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:58.585+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:17:58.594+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:17:58.597+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.595+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 28, 586431, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:17:58.597+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.597+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:58.837+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.837+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 28, 835349, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:58.837+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.837+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:58.889+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.889+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 28, 888383, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:17:58.889+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:17:58.889+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:17:58.890+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:18:28.992+0000] {processor.py:153} INFO - Started process (PID=620) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:18:28.993+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:18:28.994+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:28.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:18:29.006+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:18:29.015+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:18:29.018+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.016+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 59, 6901, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:18:29.018+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.018+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:18:29.075+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.074+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 59, 73951, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:18:29.075+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.075+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:18:29.590+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.589+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 17, 59, 588864, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:18:29.590+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:18:29.590+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:18:29.591+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:00.102+0000] {processor.py:153} INFO - Started process (PID=627) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:00.103+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:19:00.103+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.103+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:00.116+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:00.126+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:19:00.129+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.127+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 18, 30, 117577, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:19:00.130+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.130+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:00.541+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.540+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 18, 30, 540166, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:00.541+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.541+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:00.682+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.682+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 18, 30, 681257, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:00.682+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:00.682+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:00.683+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:52.911+0000] {processor.py:153} INFO - Started process (PID=637) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:52.911+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:19:52.912+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:52.912+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:52.925+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:52.935+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2025-09-03T01:19:52.938+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:52.936+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 19, 22, 926632, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2025-09-03T01:19:52.938+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:52.938+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:02.769+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:02.768+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 18, 32, 767860, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:02.769+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:02.769+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:03.323+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:03.322+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/dag_dataquality.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 628, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 145, in write_dag
    session.query(literal(True))
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'institution_detail_data_quality_pipeline', 'last_updated_1': datetime.datetime(2025, 9, 3, 1, 18, 33, 321862, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:03.323+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:03.323+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:03.324+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 155, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/usr/local/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 659, in sync_to_db
    DAG.bulk_write_to_db(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.processor_subdir AS dag_processor_subdir, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.timetable_description AS dag_timetable_description, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.has_import_errors AS dag_has_import_errors, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id, dag_schedule_dataset_reference_1.dataset_id AS dag_schedule_dataset_reference_1_dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_schedule_dataset_reference_1_dag_id, dag_schedule_dataset_reference_1.created_at AS dag_schedule_dataset_reference_1_created_at, dag_schedule_dataset_reference_1.updated_at AS dag_schedule_dataset_reference_1_updated_at, task_outlet_dataset_reference_1.dataset_id AS task_outlet_dataset_reference_1_dataset_id, task_outlet_dataset_reference_1.dag_id AS task_outlet_dataset_reference_1_dag_id, task_outlet_dataset_reference_1.task_id AS task_outlet_dataset_reference_1_task_id, task_outlet_dataset_reference_1.created_at AS task_outlet_dataset_reference_1_created_at, task_outlet_dataset_reference_1.updated_at AS task_outlet_dataset_reference_1_updated_at 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_1_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_1_1': 'institution_detail_data_quality_pipeline'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2025-09-03T01:19:57.932+0000] {processor.py:153} INFO - Started process (PID=647) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:57.933+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:19:57.933+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:57.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:57.946+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:19:57.976+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:57.976+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:19:57.988+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:19:57.988+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:19:57.999+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.068 seconds
[2025-09-03T01:21:13.620+0000] {processor.py:153} INFO - Started process (PID=654) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:13.623+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:21:13.627+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:13.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:13.681+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:13.878+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:13.878+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:21:13.945+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:13.945+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:21:14.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.426 seconds
[2025-09-03T01:21:44.218+0000] {processor.py:153} INFO - Started process (PID=679) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:44.219+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:21:44.219+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:44.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:44.232+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:44.247+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:44.247+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:21:44.261+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:44.261+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:21:44.271+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T01:21:59.680+0000] {processor.py:153} INFO - Started process (PID=229) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:59.681+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:21:59.682+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:59.819+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:21:59.896+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.896+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:21:59.902+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.902+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:21:59.907+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.907+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:21:59.907+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.907+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:21:59.915+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.914+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T01:21:59.920+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:21:59.920+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:21:59.933+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.259 seconds
[2025-09-03T01:22:30.037+0000] {processor.py:153} INFO - Started process (PID=256) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:22:30.038+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:22:30.038+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:22:30.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:22:30.054+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:22:30.075+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:22:30.075+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:22:30.094+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:22:30.094+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:22:30.107+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T01:23:00.165+0000] {processor.py:153} INFO - Started process (PID=266) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:00.166+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:23:00.167+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:00.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:00.183+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:00.201+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:00.201+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:23:00.214+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:00.214+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:23:00.226+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:23:30.339+0000] {processor.py:153} INFO - Started process (PID=276) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:30.340+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:23:30.340+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:30.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:30.354+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:23:30.370+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:30.370+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:23:30.383+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:23:30.383+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:23:30.394+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T01:24:47.793+0000] {processor.py:153} INFO - Started process (PID=286) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:47.793+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:24:47.793+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:47.793+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:47.807+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:47.821+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:47.821+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:24:47.834+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:47.834+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:24:47.848+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T01:24:04.908+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:04.909+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:24:04.909+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:04.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:05.039+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:05.154+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.154+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:24:05.161+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.161+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:24:05.165+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.165+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:24:05.165+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.165+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:24:05.173+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.173+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T01:24:05.179+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:05.179+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:24:05.188+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.284 seconds
[2025-09-03T01:24:35.286+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:35.286+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:24:35.287+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:35.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:35.303+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:24:35.325+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:35.325+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:24:35.344+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:24:35.343+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:24:35.358+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T01:25:05.494+0000] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:25:05.494+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:25:05.495+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:05.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:25:05.514+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:25:05.533+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:05.533+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:25:05.547+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:05.547+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:25:05.558+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T01:26:02.901+0000] {processor.py:153} INFO - Started process (PID=232) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:02.901+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:26:02.902+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:02.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:02.920+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:02.939+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:02.939+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:26:02.954+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:02.953+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:26:02.965+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.068 seconds
[2025-09-03T01:26:33.099+0000] {processor.py:153} INFO - Started process (PID=242) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:33.100+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:26:33.100+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:33.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:33.120+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:25:42.833+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:42.833+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:25:42.847+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:42.847+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:25:42.859+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T01:26:38.124+0000] {processor.py:153} INFO - Started process (PID=252) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:38.125+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:26:38.125+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:38.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:38.138+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:25:47.850+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:47.850+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:25:47.862+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:25:47.862+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:25:47.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.054 seconds
[2025-09-03T01:26:58.265+0000] {processor.py:153} INFO - Started process (PID=262) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:58.266+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:26:58.267+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:58.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:58.293+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:07.942+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:07.942+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:26:07.964+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:07.964+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:26:07.979+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.095 seconds
[2025-09-03T01:26:38.063+0000] {processor.py:153} INFO - Started process (PID=273) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:38.064+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:26:38.064+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:38.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:38.077+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:26:38.092+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:38.092+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:26:38.105+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:26:38.105+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:26:38.116+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.054 seconds
[2025-09-03T01:27:08.248+0000] {processor.py:153} INFO - Started process (PID=283) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:08.249+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:27:08.249+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:08.249+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:08.263+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:08.280+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:08.279+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:27:08.294+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:08.294+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:27:08.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T01:27:38.364+0000] {processor.py:153} INFO - Started process (PID=293) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:38.365+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:27:38.365+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:38.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:38.380+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:27:38.397+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:38.397+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:27:38.411+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:27:38.411+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:27:38.424+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T01:28:08.538+0000] {processor.py:153} INFO - Started process (PID=303) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:08.539+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:28:08.539+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:08.539+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:08.551+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:08.566+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:08.566+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:28:08.581+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:08.581+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:28:08.592+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T01:28:38.638+0000] {processor.py:153} INFO - Started process (PID=313) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:38.638+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:28:38.639+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:38.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:38.653+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:28:38.670+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:38.670+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:28:38.684+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:28:38.684+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:28:38.696+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:29:08.847+0000] {processor.py:153} INFO - Started process (PID=323) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:29:08.848+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:29:08.848+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:29:08.848+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:29:08.862+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:29:08.878+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:29:08.878+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:29:08.891+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:29:08.891+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:29:08.902+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:30:13.278+0000] {processor.py:153} INFO - Started process (PID=333) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:13.279+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:30:13.279+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:13.279+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:13.293+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:13.310+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:13.310+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:30:13.324+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:13.324+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:30:13.336+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:30:43.394+0000] {processor.py:153} INFO - Started process (PID=343) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:43.394+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:30:43.395+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:43.395+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:43.410+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:30:43.428+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:43.428+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:30:43.443+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:30:43.443+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:30:43.455+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:42:17.677+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:17.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:42:17.679+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:17.828+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:17.934+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.934+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:42:17.941+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.940+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:42:17.948+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.948+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T01:42:17.948+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.948+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:42:17.959+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.959+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T01:42:17.969+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:17.969+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:42:17.984+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.310 seconds
[2025-09-03T01:42:48.093+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:48.097+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:42:48.097+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:48.097+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:48.115+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:42:48.138+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:48.138+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:42:48.155+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:42:48.155+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:42:48.168+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.078 seconds
[2025-09-03T01:43:18.213+0000] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:43:18.213+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:43:18.214+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:43:18.214+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:43:18.234+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:43:18.253+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:43:18.253+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:43:18.288+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:43:18.288+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:43:18.301+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.092 seconds
[2025-09-03T01:44:09.581+0000] {processor.py:153} INFO - Started process (PID=232) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:09.582+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:44:09.582+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:09.582+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:09.603+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:09.618+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:09.618+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:44:09.631+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:09.631+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:44:09.642+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:44:39.829+0000] {processor.py:153} INFO - Started process (PID=242) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:39.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:44:39.844+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:39.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:43:49.716+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:43:49.833+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:43:49.833+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:43:49.899+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:43:49.899+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:43:49.930+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.414 seconds
[2025-09-03T01:44:20.089+0000] {processor.py:153} INFO - Started process (PID=252) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:20.091+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:44:20.091+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:20.091+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:20.194+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:20.296+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:20.296+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:44:20.372+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:20.372+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:44:20.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.303 seconds
[2025-09-03T01:45:14.722+0000] {processor.py:153} INFO - Started process (PID=262) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:14.722+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:45:14.726+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:14.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:14.786+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:14.810+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:14.810+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:45:14.828+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:14.828+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:45:14.849+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.132 seconds
[2025-09-03T01:45:44.963+0000] {processor.py:153} INFO - Started process (PID=272) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:44.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:45:44.964+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:44.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:54.679+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:44:54.720+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:54.720+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:44:54.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:44:54.738+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:44:54.753+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.103 seconds
[2025-09-03T01:45:24.874+0000] {processor.py:153} INFO - Started process (PID=282) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:24.874+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:45:24.875+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:24.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:24.902+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:24.950+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:24.950+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:45:25.010+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:25.009+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:45:25.025+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.156 seconds
[2025-09-03T01:45:55.145+0000] {processor.py:153} INFO - Started process (PID=293) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:55.145+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:45:55.146+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:55.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:55.160+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:45:55.189+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:55.189+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:45:55.201+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:45:55.201+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:45:55.215+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.086 seconds
[2025-09-03T01:46:25.328+0000] {processor.py:153} INFO - Started process (PID=309) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:25.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:46:25.329+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:25.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:25.343+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:25.360+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:25.360+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:46:25.374+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:25.374+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:46:25.387+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T01:46:55.517+0000] {processor.py:153} INFO - Started process (PID=325) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:55.518+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:46:55.518+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:55.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:55.534+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:46:55.552+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:55.552+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:46:55.566+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:46:55.566+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:46:55.581+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T01:47:25.629+0000] {processor.py:153} INFO - Started process (PID=335) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:47:25.630+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:47:25.630+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:47:25.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:47:25.643+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:47:25.659+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:47:25.659+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:47:25.672+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:47:25.672+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:47:25.686+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T01:48:20.025+0000] {processor.py:153} INFO - Started process (PID=345) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:20.028+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:48:20.028+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:20.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:20.041+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:20.055+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:20.055+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:48:20.067+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:20.067+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:48:20.080+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:48:50.142+0000] {processor.py:153} INFO - Started process (PID=355) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:50.143+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:48:50.143+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:50.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:50.157+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:48:50.173+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:50.173+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:48:50.187+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:48:50.187+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:48:50.200+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T01:49:21.131+0000] {processor.py:153} INFO - Started process (PID=366) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:21.134+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:49:21.134+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:21.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:21.148+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:21.165+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:21.165+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:49:21.178+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:21.178+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:49:21.190+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T01:49:51.238+0000] {processor.py:153} INFO - Started process (PID=376) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:51.239+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:49:51.239+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:51.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:51.252+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:49:51.269+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:51.268+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:49:51.282+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:49:51.282+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:49:51.294+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T01:50:21.363+0000] {processor.py:153} INFO - Started process (PID=386) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:21.364+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:50:21.365+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:21.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:21.379+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:21.395+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:21.395+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:50:21.408+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:21.408+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:50:21.419+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:50:51.478+0000] {processor.py:153} INFO - Started process (PID=396) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:51.479+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:50:51.479+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:51.479+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:51.492+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:50:51.508+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:51.508+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:50:51.521+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:50:51.521+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:50:51.533+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:51:21.653+0000] {processor.py:153} INFO - Started process (PID=406) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:21.654+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:51:21.654+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:21.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:21.671+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:21.689+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:21.689+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:51:21.707+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:21.707+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:51:21.718+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T01:51:52.438+0000] {processor.py:153} INFO - Started process (PID=1628) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:52.439+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:51:52.440+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:52.440+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:52.511+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:51:52.568+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:52.568+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:51:52.603+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:51:52.602+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:51:52.614+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.180 seconds
[2025-09-03T01:52:08.679+0000] {processor.py:153} INFO - Started process (PID=2015) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:08.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:52:08.680+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:08.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:08.695+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:08.707+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:08.707+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7f62f47b9b80>
[2025-09-03T01:52:08.714+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T01:52:08.727+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:08.727+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:52:08.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:08.738+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:52:08.748+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.071 seconds
[2025-09-03T01:52:38.855+0000] {processor.py:153} INFO - Started process (PID=2025) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:38.855+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:52:38.856+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:38.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:38.869+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:52:38.883+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:38.883+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:52:38.896+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:52:38.896+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:52:38.906+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.053 seconds
[2025-09-03T01:53:09.004+0000] {processor.py:153} INFO - Started process (PID=2035) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:09.005+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:53:09.005+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:09.005+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:09.020+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:09.036+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:09.036+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:53:09.052+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:09.052+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:53:09.065+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T01:53:39.165+0000] {processor.py:153} INFO - Started process (PID=2052) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:39.166+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:53:39.166+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:39.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:39.183+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:53:39.201+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:39.201+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:53:39.219+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:53:39.218+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:53:39.231+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T01:54:09.280+0000] {processor.py:153} INFO - Started process (PID=2062) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:54:09.280+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:54:09.281+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:54:09.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:54:09.294+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:54:09.309+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:54:09.308+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:54:09.322+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:54:09.322+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:54:09.332+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.054 seconds
[2025-09-03T01:55:25.755+0000] {processor.py:153} INFO - Started process (PID=2079) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:25.755+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:55:25.756+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:25.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:25.771+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:25.789+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:25.789+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:55:25.804+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:25.804+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:55:25.815+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T01:55:55.940+0000] {processor.py:153} INFO - Started process (PID=2089) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:55.941+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:55:55.941+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:55.941+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:55.955+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:55:55.969+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:55.969+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:55:55.982+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:55:55.982+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:55:55.993+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T01:56:26.982+0000] {processor.py:153} INFO - Started process (PID=2100) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:26.982+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:56:26.983+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:26.983+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:26.996+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:27.010+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:27.010+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:56:27.025+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:27.025+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:56:27.035+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T01:56:57.149+0000] {processor.py:153} INFO - Started process (PID=2110) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:57.150+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:56:57.150+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:57.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:57.163+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:56:57.178+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:57.178+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:56:57.191+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:56:57.191+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:56:57.202+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T01:57:27.318+0000] {processor.py:153} INFO - Started process (PID=2120) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:27.319+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:57:27.319+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:27.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:27.336+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:27.354+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:27.354+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:57:27.374+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:27.374+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:57:27.387+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.071 seconds
[2025-09-03T01:57:57.517+0000] {processor.py:153} INFO - Started process (PID=2130) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:57.518+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:57:57.518+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:57.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:57.534+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:57:57.551+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:57.551+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:57:57.567+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:57:57.567+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:57:57.580+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T01:58:27.626+0000] {processor.py:153} INFO - Started process (PID=2140) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:27.629+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:58:27.629+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:27.629+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:27.643+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:27.659+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:27.659+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:58:27.671+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:27.671+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:58:27.682+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T01:58:57.812+0000] {processor.py:153} INFO - Started process (PID=2150) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:57.813+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:58:57.813+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:57.813+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:57.826+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:58:57.841+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:57.841+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:58:57.854+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:58:57.854+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:58:57.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T01:59:51.182+0000] {processor.py:153} INFO - Started process (PID=2160) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:59:51.183+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T01:59:51.183+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:59:51.183+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:59:51.196+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T01:59:51.212+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:59:51.212+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T01:59:51.225+0000] {logging_mixin.py:137} INFO - [2025-09-03T01:59:51.225+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T01:59:51.238+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.058 seconds
[2025-09-03T02:00:21.356+0000] {processor.py:153} INFO - Started process (PID=2171) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:21.356+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:00:21.356+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:21.356+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:21.369+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:21.386+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:21.386+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:00:21.400+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:21.400+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:00:21.411+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T02:00:52.409+0000] {processor.py:153} INFO - Started process (PID=2182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:52.410+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:00:52.411+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:52.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:52.427+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:00:52.444+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:52.444+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:00:52.459+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:00:52.459+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:00:52.471+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.064 seconds
[2025-09-03T02:01:22.533+0000] {processor.py:153} INFO - Started process (PID=2330) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:22.533+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:01:22.534+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:22.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:22.552+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:22.575+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:22.575+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:01:22.596+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:22.596+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:01:22.611+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.082 seconds
[2025-09-03T02:02:41.424+0000] {processor.py:153} INFO - Started process (PID=3750) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:02:41.425+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:02:41.426+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:02:41.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:02:41.446+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:02:41.469+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:02:41.468+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:02:41.486+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:02:41.486+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:02:41.500+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.077 seconds
[2025-09-03T02:01:56.036+0000] {processor.py:153} INFO - Started process (PID=3782) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:56.036+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:01:56.037+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:56.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:56.050+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:01:56.061+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:56.060+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7f62f47c7ee0>
[2025-09-03T02:01:56.066+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:01:56.082+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:56.082+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:01:56.092+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:01:56.092+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:01:56.102+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.068 seconds
[2025-09-03T02:03:31.493+0000] {processor.py:153} INFO - Started process (PID=3787) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:03:31.494+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:03:31.494+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:03:31.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:03:31.516+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:03:31.533+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:03:31.533+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:03:31.547+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:03:31.547+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:03:31.558+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:04:01.598+0000] {processor.py:153} INFO - Started process (PID=3798) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:01.598+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:04:01.599+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:01.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:01.611+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:01.633+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:01.632+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:04:01.647+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:01.647+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:04:01.659+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T02:04:31.779+0000] {processor.py:153} INFO - Started process (PID=3808) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:31.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:04:31.780+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:31.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:31.793+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:03:41.509+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:03:41.509+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:03:41.524+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:03:41.524+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:03:41.535+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T02:04:11.661+0000] {processor.py:153} INFO - Started process (PID=3818) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:11.661+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:04:11.661+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:11.661+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:11.674+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:11.690+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:11.690+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:04:11.704+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:11.704+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:04:11.714+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T02:04:41.824+0000] {processor.py:153} INFO - Started process (PID=3829) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:41.827+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:04:41.828+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:41.827+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:41.841+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:04:41.858+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:41.857+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:04:41.870+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:04:41.870+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:04:41.881+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T02:05:21.922+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:21.923+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:05:21.923+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:21.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:22.090+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:22.243+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.242+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T02:05:22.253+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.253+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T02:05:22.261+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.260+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T02:05:22.261+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.261+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:05:22.274+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.274+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T02:05:22.282+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:22.282+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:05:22.298+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.379 seconds
[2025-09-03T02:05:52.401+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:52.402+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:05:52.403+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:52.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:52.428+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:05:52.458+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:52.458+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:05:52.483+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:05:52.483+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:05:52.502+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.104 seconds
[2025-09-03T02:06:46.832+0000] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:46.833+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:06:46.834+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:46.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:46.850+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:46.869+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:46.869+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:06:46.884+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:46.884+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:06:46.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T02:07:17.010+0000] {processor.py:153} INFO - Started process (PID=232) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:07:17.011+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:07:17.012+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:07:17.012+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:26.763+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:26.825+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:26.825+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:06:26.846+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:26.846+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:06:26.859+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.161 seconds
[2025-09-03T02:06:31.737+0000] {processor.py:153} INFO - Started process (PID=242) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:31.738+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:06:31.739+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:31.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:31.764+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:31.792+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:31.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:06:31.816+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:31.816+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:06:31.834+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.100 seconds
[2025-09-03T02:07:47.094+0000] {processor.py:153} INFO - Started process (PID=468) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:07:47.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:07:47.097+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:07:47.097+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:57.139+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:06:58.191+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:58.190+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:06:58.230+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:06:58.229+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:06:58.249+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 1.463 seconds
[2025-09-03T02:07:28.377+0000] {processor.py:153} INFO - Started process (PID=1651) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:07:28.378+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:07:28.379+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:07:28.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:07:28.401+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:07:28.427+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:07:28.427+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:07:28.448+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:07:28.448+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:07:28.465+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.092 seconds
[2025-09-03T02:08:01.247+0000] {processor.py:153} INFO - Started process (PID=2493) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:08:52.013+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:08:03.318+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:08:02.807+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:08:24.026+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:08:24.355+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:08:24.354+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:08:24.571+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:08:24.571+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:08:24.682+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 24.342 seconds
[2025-09-03T02:09:17.054+0000] {processor.py:153} INFO - Started process (PID=2646) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:17.056+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:09:17.057+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:09:17.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:17.121+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:17.158+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:09:17.157+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:08:26.916+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:08:26.916+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:08:26.948+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.197 seconds
[2025-09-03T02:09:39.678+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:39.688+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:09:39.689+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:09:39.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:39.905+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:09:39.942+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:09:39.942+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:09:39.970+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:09:39.969+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:09:39.989+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.315 seconds
[2025-09-03T02:10:10.198+0000] {processor.py:153} INFO - Started process (PID=560) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:10.198+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:10:10.199+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:10.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:10.211+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:10.227+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:10.227+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:10:10.240+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:10.239+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:10:10.251+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T02:10:40.358+0000] {processor.py:153} INFO - Started process (PID=571) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:40.359+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:10:40.360+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:40.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:40.379+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:10:40.419+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:40.419+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:10:40.437+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:10:40.437+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:10:40.452+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.096 seconds
[2025-09-03T02:11:10.569+0000] {processor.py:153} INFO - Started process (PID=581) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:10.570+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:11:10.571+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:10.570+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:10.602+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:10.637+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:10.637+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:11:10.671+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:10.671+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:11:10.693+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.126 seconds
[2025-09-03T02:12:04.023+0000] {processor.py:153} INFO - Started process (PID=586) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:04.023+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:12:04.024+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:04.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:04.040+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:04.058+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:04.058+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:12:04.075+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:04.075+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:12:04.089+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:11:43.936+0000] {processor.py:153} INFO - Started process (PID=596) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:43.938+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:11:43.938+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:43.938+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:43.997+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:11:44.029+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:44.029+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:11:44.053+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:11:44.053+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:11:44.071+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.137 seconds
[2025-09-03T02:12:14.188+0000] {processor.py:153} INFO - Started process (PID=607) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:14.189+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:12:14.189+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:14.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:14.203+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:14.223+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:14.223+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:12:14.238+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:14.238+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:12:14.252+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T02:12:44.287+0000] {processor.py:153} INFO - Started process (PID=617) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:44.287+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:12:44.288+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:44.288+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:44.302+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:12:44.319+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:44.319+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:12:44.335+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:12:44.335+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:12:44.347+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
[2025-09-03T02:13:14.472+0000] {processor.py:153} INFO - Started process (PID=627) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:14.473+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:13:14.473+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:14.473+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:14.489+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:14.504+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:14.504+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:13:14.516+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:14.516+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:13:14.527+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T02:13:44.639+0000] {processor.py:153} INFO - Started process (PID=637) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:44.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:13:44.640+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:44.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:44.655+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:13:44.672+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:44.672+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:13:44.687+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:13:44.687+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:13:44.700+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T02:14:14.814+0000] {processor.py:153} INFO - Started process (PID=647) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:14.814+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:14:14.815+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:14.815+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:14.827+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:14.860+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:14.860+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:14:14.872+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:14.872+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:14:14.884+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T02:14:44.925+0000] {processor.py:153} INFO - Started process (PID=658) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:44.926+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:14:44.926+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:44.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:44.938+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:14:44.953+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:44.952+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:14:44.965+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:14:44.965+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:14:44.976+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.053 seconds
[2025-09-03T02:15:44.317+0000] {processor.py:153} INFO - Started process (PID=668) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:15:44.318+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:15:44.318+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:15:44.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:15:44.330+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:15:44.345+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:15:44.345+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:15:44.358+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:15:44.358+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:15:44.369+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T02:16:14.485+0000] {processor.py:153} INFO - Started process (PID=678) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:14.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:16:14.487+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:14.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:14.508+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:14.559+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:14.558+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:16:14.576+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:14.576+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:16:14.591+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.108 seconds
[2025-09-03T02:16:49.623+0000] {processor.py:153} INFO - Started process (PID=689) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:49.625+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:16:49.625+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:49.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:15:59.362+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:15:59.377+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:15:59.377+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:15:59.405+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:15:59.405+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:15:59.419+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.096 seconds
[2025-09-03T02:16:29.507+0000] {processor.py:153} INFO - Started process (PID=699) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:29.508+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:16:29.508+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:29.508+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:29.533+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:29.547+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:29.547+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:16:29.567+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:29.567+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:16:29.578+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T02:16:59.694+0000] {processor.py:153} INFO - Started process (PID=710) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:59.697+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:16:59.697+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:59.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:59.726+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:16:59.759+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:59.759+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:16:59.774+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:16:59.774+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:16:59.785+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.094 seconds
[2025-09-03T02:17:29.897+0000] {processor.py:153} INFO - Started process (PID=720) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:17:29.898+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:17:29.898+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:17:29.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:17:29.914+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:17:29.932+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:17:29.931+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:17:29.947+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:17:29.947+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:17:29.963+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:18:00.077+0000] {processor.py:153} INFO - Started process (PID=730) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:00.078+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:18:00.079+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:00.078+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:00.095+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:00.117+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:00.117+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:18:00.131+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:00.131+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:18:00.146+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T02:18:56.482+0000] {processor.py:153} INFO - Started process (PID=740) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:56.483+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:18:56.483+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:56.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:56.499+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:18:56.516+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:56.515+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:18:56.532+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:18:56.532+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:18:56.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T02:19:29.667+0000] {processor.py:153} INFO - Started process (PID=751) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:29.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:19:29.668+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:29.668+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:29.683+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:29.700+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:29.700+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:19:29.714+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:29.714+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:19:29.725+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T02:19:59.846+0000] {processor.py:153} INFO - Started process (PID=761) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:59.847+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:19:59.847+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:59.847+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:59.870+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:59.896+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:59.896+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:19:09.630+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:09.630+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:19:09.647+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T02:19:39.777+0000] {processor.py:153} INFO - Started process (PID=771) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:39.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:19:39.779+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:39.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:39.818+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:19:39.871+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:39.870+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:19:39.886+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:19:39.886+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:19:39.900+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.132 seconds
[2025-09-03T02:20:09.992+0000] {processor.py:153} INFO - Started process (PID=784) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:09.993+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:20:09.993+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:09.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:10.008+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:10.022+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:10.022+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:20:10.035+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:10.035+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:20:10.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T02:20:40.134+0000] {processor.py:153} INFO - Started process (PID=791) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:40.135+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:20:40.135+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:40.135+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:40.175+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:20:40.203+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:40.203+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:20:40.218+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:20:40.218+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:20:40.230+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T02:21:10.296+0000] {processor.py:153} INFO - Started process (PID=801) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:10.297+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:21:10.298+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:10.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:10.342+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:10.375+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:10.375+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:21:10.390+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:10.389+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:21:10.401+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.108 seconds
[2025-09-03T02:21:40.436+0000] {processor.py:153} INFO - Started process (PID=811) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:40.439+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:21:40.440+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:40.440+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:40.458+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:21:40.481+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:40.481+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:21:40.526+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:21:40.525+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:21:40.543+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.109 seconds
[2025-09-03T02:22:11.460+0000] {processor.py:153} INFO - Started process (PID=1953) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:11.461+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:22:11.461+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:11.461+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:11.683+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:11.809+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:11.809+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:22:11.907+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:11.907+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:22:11.935+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.503 seconds
[2025-09-03T02:22:13.345+0000] {processor.py:153} INFO - Started process (PID=2102) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:13.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:22:13.346+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:13.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:13.363+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:13.375+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:13.375+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7fb15b1bedc0>
[2025-09-03T02:22:13.407+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:22:13.435+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:13.434+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:22:13.446+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:13.446+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:22:13.458+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.115 seconds
[2025-09-03T02:22:29.299+0000] {processor.py:153} INFO - Started process (PID=2478) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:29.300+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:22:29.300+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:29.300+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:29.317+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:29.331+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:29.330+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7fb15b1c0e50>
[2025-09-03T02:22:29.337+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:22:29.357+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:29.356+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:22:29.367+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:29.367+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:22:29.379+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.081 seconds
[2025-09-03T02:22:59.474+0000] {processor.py:153} INFO - Started process (PID=2488) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:59.475+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:22:59.475+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:59.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:59.490+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:22:59.504+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:59.504+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:22:59.517+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:22:59.516+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:22:59.527+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T02:23:29.610+0000] {processor.py:153} INFO - Started process (PID=2498) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:29.610+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:23:29.611+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:29.611+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:29.625+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:29.640+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:29.640+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:23:29.654+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:29.654+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:23:29.667+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T02:23:44.752+0000] {processor.py:153} INFO - Started process (PID=2543) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:44.753+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:23:44.753+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:44.753+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:44.768+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:23:44.779+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:44.779+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_success at 0x7fb15b1bfaf0>
[2025-09-03T02:23:44.785+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:23:44.803+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:44.803+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:23:44.814+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:23:44.814+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:24:35.107+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T02:25:05.239+0000] {processor.py:153} INFO - Started process (PID=2553) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:05.240+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:25:05.240+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:05.240+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:05.256+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:05.316+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:05.316+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:25:05.332+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:05.332+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:25:05.346+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.109 seconds
[2025-09-03T02:25:35.425+0000] {processor.py:153} INFO - Started process (PID=2563) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:35.426+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:25:35.426+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:35.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:35.445+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:24:45.172+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:24:45.172+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:24:45.190+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:24:45.190+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:24:45.203+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.066 seconds
[2025-09-03T02:25:15.316+0000] {processor.py:153} INFO - Started process (PID=2624) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:15.317+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:25:15.317+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:15.317+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:15.332+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:15.343+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:15.343+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:25:15.358+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:15.358+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:25:15.370+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T02:25:45.492+0000] {processor.py:153} INFO - Started process (PID=2640) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:45.493+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:25:45.493+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:45.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:45.508+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:25:45.522+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:45.522+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:25:45.535+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:25:45.535+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:25:45.545+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.055 seconds
[2025-09-03T02:26:15.651+0000] {processor.py:153} INFO - Started process (PID=2656) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:15.652+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:26:15.653+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:15.653+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:15.671+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:15.688+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:15.688+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:26:15.701+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:15.701+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:26:15.712+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.064 seconds
[2025-09-03T02:26:45.764+0000] {processor.py:153} INFO - Started process (PID=2704) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:45.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:26:45.765+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:45.765+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:45.781+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:26:45.798+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:45.798+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:26:45.813+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:26:45.813+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:26:45.825+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T02:27:15.944+0000] {processor.py:153} INFO - Started process (PID=2714) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:15.945+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:27:15.945+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:15.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:15.961+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:15.976+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:15.976+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:27:15.991+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:15.991+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:27:16.002+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T02:27:46.122+0000] {processor.py:153} INFO - Started process (PID=2725) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:46.122+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:27:46.123+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:46.123+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:46.145+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:27:46.167+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:46.167+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:27:46.185+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:27:46.185+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:27:46.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.079 seconds
[2025-09-03T02:28:50.549+0000] {processor.py:153} INFO - Started process (PID=2805) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:50.549+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:28:50.550+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:50.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:50.569+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:50.588+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:50.588+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:28:50.608+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:50.608+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:28:50.627+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.081 seconds
[2025-09-03T02:28:03.365+0000] {processor.py:153} INFO - Started process (PID=2823) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:03.366+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:28:03.366+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:03.366+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:03.382+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:03.395+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:03.394+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_fail at 0x7fb15b1c1700>
[2025-09-03T02:28:03.421+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:28:03.934+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:03.934+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:28:03.946+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:03.946+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:28:03.957+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.594 seconds
[2025-09-03T02:28:55.679+0000] {processor.py:153} INFO - Started process (PID=2828) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:55.680+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:28:55.682+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:55.681+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:55.707+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:55.729+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:55.728+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_fail at 0x7fb15b1c1700>
[2025-09-03T02:28:55.738+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:28:11.305+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:11.305+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:28:11.317+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:11.317+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:28:11.329+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 5.931 seconds
[2025-09-03T02:29:05.630+0000] {processor.py:153} INFO - Started process (PID=2833) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:05.631+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:29:05.631+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:05.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:05.646+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:05.661+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:05.661+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:29:05.675+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:05.675+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:29:05.686+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T02:28:25.519+0000] {processor.py:153} INFO - Started process (PID=2877) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:25.520+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:28:25.520+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:25.520+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:25.536+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:25.548+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:25.548+0000] {dag.py:1318} INFO - Executing dag callback function: <function log_callback_fail at 0x7fb15b1c3820>
[2025-09-03T02:28:25.554+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/airflow/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T02:28:26.024+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:26.024+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:28:26.035+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:26.035+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:28:26.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.530 seconds
[2025-09-03T02:28:56.175+0000] {processor.py:153} INFO - Started process (PID=2882) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:56.175+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:28:56.176+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:56.176+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:56.195+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:28:56.214+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:56.213+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:28:56.229+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:28:56.229+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:28:56.242+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:29:26.341+0000] {processor.py:153} INFO - Started process (PID=2892) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:26.342+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:29:26.342+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:26.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:26.365+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:26.386+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:26.385+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:29:26.402+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:26.402+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:29:26.416+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.079 seconds
[2025-09-03T02:30:17.737+0000] {processor.py:153} INFO - Started process (PID=2897) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:17.738+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:30:17.738+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:17.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:17.755+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:17.775+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:17.775+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:30:17.791+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:17.791+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:30:17.804+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:30:49.935+0000] {processor.py:153} INFO - Started process (PID=2907) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:49.936+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:30:49.937+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:49.937+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:59.679+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:29:59.698+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:59.697+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:29:59.712+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:29:59.712+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:29:59.724+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T02:30:29.769+0000] {processor.py:153} INFO - Started process (PID=2917) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:29.770+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:30:29.770+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:29.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:29.787+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:29.805+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:29.805+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:30:29.822+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:29.822+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:30:29.835+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.069 seconds
[2025-09-03T02:30:59.973+0000] {processor.py:153} INFO - Started process (PID=2927) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:59.974+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:30:59.974+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:30:59.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:30:59.992+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:31:00.009+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:31:00.009+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:31:00.024+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:31:00.024+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:31:00.035+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.064 seconds
[2025-09-03T02:31:30.075+0000] {processor.py:153} INFO - Started process (PID=2937) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:31:30.075+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:31:30.076+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:31:30.075+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:31:30.090+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:31:30.107+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:31:30.107+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:31:30.121+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:31:30.121+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:31:30.132+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T02:32:00.250+0000] {processor.py:153} INFO - Started process (PID=2947) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:00.250+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:32:00.251+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:00.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:00.268+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:00.286+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:00.286+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:32:00.301+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:00.301+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:32:00.313+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T02:32:30.416+0000] {processor.py:153} INFO - Started process (PID=2957) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:30.417+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:32:30.417+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:30.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:30.432+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:32:30.447+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:30.447+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:32:30.461+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:32:30.461+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:32:30.472+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T02:33:00.557+0000] {processor.py:153} INFO - Started process (PID=2967) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:00.560+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:33:00.560+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:00.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:00.587+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:00.619+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:00.618+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:33:00.660+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:00.660+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:33:00.674+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.132 seconds
[2025-09-03T02:34:06.031+0000] {processor.py:153} INFO - Started process (PID=2978) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:06.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:34:06.033+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:06.033+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:06.066+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:06.094+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:06.094+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:34:06.131+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:06.131+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:34:06.153+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.125 seconds
[2025-09-03T02:33:46.022+0000] {processor.py:153} INFO - Started process (PID=2989) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:46.028+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:33:46.029+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:46.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:46.109+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:46.155+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:46.155+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:33:46.186+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:46.186+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:33:46.283+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.227 seconds
[2025-09-03T02:33:56.038+0000] {processor.py:153} INFO - Started process (PID=2999) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:56.039+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:33:56.039+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:56.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:56.059+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:33:56.087+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:56.086+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:33:56.102+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:33:56.102+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:33:56.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.084 seconds
[2025-09-03T02:34:26.245+0000] {processor.py:153} INFO - Started process (PID=3009) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:26.246+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:34:26.246+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:26.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:26.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:26.284+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:26.284+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:34:26.299+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:26.299+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:34:26.313+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.071 seconds
[2025-09-03T02:34:56.444+0000] {processor.py:153} INFO - Started process (PID=3019) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:56.446+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:34:56.447+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:56.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:56.492+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:34:56.525+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:56.525+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:34:56.551+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:34:56.551+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:34:56.581+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.142 seconds
[2025-09-03T02:35:26.704+0000] {processor.py:153} INFO - Started process (PID=3029) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:26.706+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:35:26.706+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:26.706+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:26.732+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:26.762+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:26.762+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:35:26.786+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:26.786+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:35:26.816+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.116 seconds
[2025-09-03T02:35:56.970+0000] {processor.py:153} INFO - Started process (PID=3039) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:56.971+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:35:56.972+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:56.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:56.994+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:35:57.021+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:57.020+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:35:57.042+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:35:57.042+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:35:57.062+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.095 seconds
[2025-09-03T02:36:51.368+0000] {processor.py:153} INFO - Started process (PID=3049) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:36:51.369+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:36:51.371+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:36:51.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:36:51.391+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:36:51.417+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:36:51.417+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:36:51.440+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:36:51.440+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:36:51.462+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.096 seconds
[2025-09-03T02:37:41.566+0000] {processor.py:153} INFO - Started process (PID=3069) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:37:41.569+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:37:41.569+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:37:41.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:36:51.700+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:37:25.955+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:37:25.955+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:37:25.999+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:37:25.999+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:37:26.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 34.774 seconds
[2025-09-03T02:38:26.412+0000] {processor.py:153} INFO - Started process (PID=3082) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:26.413+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:38:26.414+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:26.414+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:26.493+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:26.541+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:26.540+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:38:26.572+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:26.571+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:38:26.602+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.196 seconds
[2025-09-03T02:39:01.582+0000] {processor.py:153} INFO - Started process (PID=3092) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:01.584+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:39:01.585+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:01.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:01.654+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:11.433+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:11.433+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:38:11.462+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:11.462+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:38:11.494+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.201 seconds
[2025-09-03T02:38:41.593+0000] {processor.py:153} INFO - Started process (PID=3102) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:41.595+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:38:41.595+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:41.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:41.673+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:38:41.701+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:41.701+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:38:41.723+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:38:41.723+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:38:41.745+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.157 seconds
[2025-09-03T02:39:11.875+0000] {processor.py:153} INFO - Started process (PID=3112) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:11.876+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:39:11.877+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:11.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:11.908+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:11.950+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:11.950+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:39:11.976+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:11.975+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:39:11.997+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.125 seconds
[2025-09-03T02:39:42.023+0000] {processor.py:153} INFO - Started process (PID=3122) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:42.025+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:39:42.025+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:42.025+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:42.059+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:39:42.084+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:42.084+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:39:42.105+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:39:42.105+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:39:42.129+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.110 seconds
[2025-09-03T02:40:12.285+0000] {processor.py:153} INFO - Started process (PID=3132) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:40:12.285+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:40:12.286+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:40:12.286+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:40:12.311+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:40:12.339+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:40:12.338+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:40:12.363+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:40:12.363+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:40:12.383+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.103 seconds
[2025-09-03T02:41:06.717+0000] {processor.py:153} INFO - Started process (PID=3142) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:06.718+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:41:06.718+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:06.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:06.742+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:06.768+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:06.768+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:41:06.791+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:06.791+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:41:06.810+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.097 seconds
[2025-09-03T02:41:36.840+0000] {processor.py:153} INFO - Started process (PID=3153) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:36.840+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:41:36.841+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:36.841+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:36.856+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:36.903+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:36.902+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:41:36.917+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:36.917+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:40:46.656+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.101 seconds
[2025-09-03T02:41:16.799+0000] {processor.py:153} INFO - Started process (PID=3163) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:16.800+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:41:16.801+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:16.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:16.833+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:16.877+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:16.876+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:41:16.897+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:16.897+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:41:16.919+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.125 seconds
[2025-09-03T02:41:54.095+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:54.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:41:54.097+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:54.097+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:54.383+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:41:54.444+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:54.444+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:41:54.464+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:41:54.464+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:41:54.481+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.397 seconds
[2025-09-03T02:42:24.502+0000] {processor.py:153} INFO - Started process (PID=214) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:42:24.502+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:42:24.503+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:42:24.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:42:24.525+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:42:24.559+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:42:24.558+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:42:24.582+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:42:24.582+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:42:24.597+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.098 seconds
[2025-09-03T02:43:16.903+0000] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:16.904+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:43:16.905+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:16.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:16.931+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:16.956+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:16.955+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:43:16.976+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:16.976+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:43:16.994+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.095 seconds
[2025-09-03T02:43:47.104+0000] {processor.py:153} INFO - Started process (PID=234) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:47.106+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:43:47.106+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:47.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:47.201+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:43:47.240+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:47.240+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:43:47.286+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:43:47.286+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:43:47.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.205 seconds
[2025-09-03T02:44:17.440+0000] {processor.py:153} INFO - Started process (PID=244) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:17.443+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:44:17.444+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:17.444+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:17.471+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:17.514+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:17.514+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:44:17.543+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:17.543+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:44:17.568+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.135 seconds
[2025-09-03T02:44:47.682+0000] {processor.py:153} INFO - Started process (PID=254) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:47.684+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:44:47.685+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:47.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:47.714+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:44:47.758+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:47.758+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:44:47.801+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:44:47.801+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:44:47.848+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.169 seconds
[2025-09-03T02:45:42.168+0000] {processor.py:153} INFO - Started process (PID=264) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:45:42.168+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T02:45:42.168+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:45:42.168+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:45:42.184+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T02:45:42.202+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:45:42.202+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T02:45:42.220+0000] {logging_mixin.py:137} INFO - [2025-09-03T02:45:42.220+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T02:45:42.240+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.075 seconds
[2025-09-03T03:28:53.168+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:28:53.170+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:28:53.171+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:28:53.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:28:53.389+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:28:53.436+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:28:53.436+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:28:53.473+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:28:53.472+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:28:53.490+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.326 seconds
[2025-09-03T03:29:23.602+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:23.602+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:29:23.603+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:23.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:23.615+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:23.632+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:23.632+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:29:23.645+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:23.645+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:29:23.656+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.056 seconds
[2025-09-03T03:29:53.768+0000] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:53.769+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:29:53.769+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:53.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:53.784+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:29:53.800+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:53.800+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:29:53.814+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:29:53.814+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:29:53.825+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.059 seconds
[2025-09-03T03:54:22.263+0000] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:22.269+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:54:22.275+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.274+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:22.580+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:22.659+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.659+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T03:54:22.665+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.665+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T03:54:22.671+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.670+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T03:54:22.671+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.671+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:54:22.679+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.678+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T03:54:22.685+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:22.685+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:54:22.695+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.444 seconds
[2025-09-03T03:54:52.798+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:52.798+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:54:52.799+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:52.799+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:52.814+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:54:52.831+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:52.831+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:54:52.847+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:54:52.846+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:54:52.857+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T03:55:22.902+0000] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:22.902+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:55:22.903+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:22.903+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:22.919+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:22.938+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:22.938+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:55:22.956+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:22.956+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:55:22.969+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.070 seconds
[2025-09-03T03:55:53.020+0000] {processor.py:153} INFO - Started process (PID=232) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:53.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:55:53.021+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:53.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:53.045+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:55:53.064+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:53.064+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:55:53.079+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:55:53.079+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:55:53.089+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.072 seconds
[2025-09-03T03:56:23.179+0000] {processor.py:153} INFO - Started process (PID=242) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:23.180+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:56:23.180+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:23.180+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:23.200+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:23.218+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:23.218+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:56:23.231+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:23.231+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:56:23.242+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.065 seconds
[2025-09-03T03:56:53.347+0000] {processor.py:153} INFO - Started process (PID=252) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:53.348+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:56:53.349+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:53.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:53.368+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:56:53.392+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:53.392+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:56:53.413+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:56:53.413+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:56:53.430+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.087 seconds
[2025-09-03T03:57:23.554+0000] {processor.py:153} INFO - Started process (PID=262) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:23.555+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:57:23.555+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:23.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:23.569+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:23.584+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:23.584+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:57:23.597+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:23.597+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:57:23.609+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.057 seconds
[2025-09-03T03:57:53.725+0000] {processor.py:153} INFO - Started process (PID=272) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:53.727+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:57:53.727+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:53.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:53.743+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:57:53.759+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:53.759+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:57:53.773+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:57:53.773+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:57:53.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.060 seconds
[2025-09-03T03:59:04.111+0000] {processor.py:153} INFO - Started process (PID=282) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:59:04.112+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T03:59:04.112+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:59:04.112+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:59:04.129+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T03:59:04.147+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:59:04.146+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T03:59:04.161+0000] {logging_mixin.py:137} INFO - [2025-09-03T03:59:04.160+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T03:59:04.172+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.063 seconds
[2025-09-03T04:32:20.060+0000] {processor.py:153} INFO - Started process (PID=185) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:20.061+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:32:20.062+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.062+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:20.236+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:20.347+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.347+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:32:20.355+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.355+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:32:20.361+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.361+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:32:20.361+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.361+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:32:20.371+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.371+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T04:32:20.379+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:20.378+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:32:20.390+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.333 seconds
[2025-09-03T04:32:50.485+0000] {processor.py:153} INFO - Started process (PID=247) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:50.485+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:32:50.486+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:50.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:50.500+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:32:50.518+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:50.518+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:32:50.532+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:32:50.532+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:32:50.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.061 seconds
[2025-09-03T04:33:20.587+0000] {processor.py:153} INFO - Started process (PID=257) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:33:20.587+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:33:20.588+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:33:20.588+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:33:20.601+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:33:20.616+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:33:20.616+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:33:20.629+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:33:20.629+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:33:20.639+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.054 seconds
[2025-09-03T04:35:59.607+0000] {processor.py:153} INFO - Started process (PID=217) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:35:59.609+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:35:59.609+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:35:59.798+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:35:59.886+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.886+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:35:59.892+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.892+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:35:59.896+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.896+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:institution_detail_data_quality_pipeline
[2025-09-03T04:35:59.896+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.896+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:35:59.904+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.904+0000] {dag.py:2711} INFO - Creating ORM DAG for institution_detail_data_quality_pipeline
[2025-09-03T04:35:59.910+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:35:59.910+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:35:59.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.315 seconds
[2025-09-03T04:37:02.197+0000] {processor.py:153} INFO - Started process (PID=247) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:02.198+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:37:02.198+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:02.198+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:02.214+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:02.233+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:02.233+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:37:02.250+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:02.249+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:37:02.262+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.067 seconds
[2025-09-03T04:37:32.385+0000] {processor.py:153} INFO - Started process (PID=255) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:32.385+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:37:32.386+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:32.386+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:32.426+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:36:42.229+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:36:42.229+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:36:42.248+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:36:42.248+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:36:42.267+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.109 seconds
[2025-09-03T04:37:12.418+0000] {processor.py:153} INFO - Started process (PID=265) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:12.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:37:12.425+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:12.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:12.483+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:12.563+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:12.563+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:37:12.588+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:12.588+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:37:12.611+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.196 seconds
[2025-09-03T04:37:42.708+0000] {processor.py:153} INFO - Started process (PID=277) to work on /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:42.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/dag_dataquality.py for tasks to queue
[2025-09-03T04:37:42.709+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:42.709+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:42.723+0000] {processor.py:753} INFO - DAG(s) dict_keys(['institution_detail_data_quality_pipeline']) retrieved from /opt/airflow/dags/dag_dataquality.py
[2025-09-03T04:37:42.740+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:42.740+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2025-09-03T04:37:42.754+0000] {logging_mixin.py:137} INFO - [2025-09-03T04:37:42.754+0000] {dag.py:3437} INFO - Setting next_dagrun for institution_detail_data_quality_pipeline to None, run_after=None
[2025-09-03T04:37:42.768+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/dag_dataquality.py took 0.062 seconds
